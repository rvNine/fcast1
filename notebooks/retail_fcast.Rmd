---
title: "Retail Sales Forecasting"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    code_folding: none
    df_print: paged
    highlight: tango
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    eval = TRUE,
    message = FALSE,
    warning = FALSE,
    dpi = 300,
    fig.align = "center"
    )

# See Options: knitr::opts_chunk$get()
```



# Introduction


The training data includes dates, store and item information, whether that item was being promoted, as well as the unit sales. Additional files include supplementary information that may be useful in building your models.

File Descriptions and Data Field Information

`train.csv`

- Training data, which includes the target unit_sales by date, store_nbr, and item_nbr and a unique id to label rows.
- The target unit_sales can be integer (e.g., a bag of chips) or float (e.g., 1.5 kg of cheese).
Negative values of unit_sales represent returns of that particular item.
- The onpromotion column tells whether that item_nbr was on promotion for a specified date and store_nbr.
- Approximately 16% of the onpromotion values in this file are NaN.
- NOTE: The training data does not include rows for items that had zero unit_sales for a store/date combination. There is no information as to whether or not the item was in stock for the store on the date, and teams will need to decide the best way to handle that situation. Also, there are a small number of items seen in the training data that aren't seen in the test data.

`stores.csv`

- Store metadata, including city, state, type, and cluster.
- cluster is a grouping of similar stores.

`items.csv`

- Item metadata, including family, class, and perishable.
- NOTE: Items marked as perishable have a score weight of 1.25; otherwise, the weight is 1.0.

`transactions.csv`

- The count of sales transactions for each date, store_nbr combination. Only included for the training data timeframe.

`oil.csv`

- Daily oil price. Includes values during both the train and test data timeframe. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)

`holidays_events.csv`

- Holidays and Events, with metadata
- NOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquilwas transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.
- Additional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).

- Additional Notes

- Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Supermarket sales could be affected by this.
- A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.



# Time Series forecasting

## libraries


```{r}
library('ggplot2')
library('dplyr') 
library('readr') 
library('data.table') 
library('forecast')
library('prophet')
library('tibble') 
library('tidyr') 
library('stringr') 
library('forcats') 
library('lubridate')
```

## Load data

 *training* data is 4.7 GB in size with 126 million rows. 
```{r warning=FALSE, results=FALSE}
set.seed(32)
## reading train data 
train_data_f <- fread('../data/raw/train.csv', skip = 36458909, 
               col.names = c('id', 'date','store_nbr', 'item_nbr', 'unit_sales', 'onpromotion'))
```


## Training data

```{r}
summary(train_data_f)
gc()
```

```{r}
glimpse(train_data_f)
```


## time series data for a store,item

Top 2 items by sales: 1503844, 1047679
Top 2 store_nbr by sales:  44, 45
Considering time series of store 44, Item 1503844
```{r}

train_data_f[, ':='(
  date = ymd(date, tz = NULL),
  store_item = paste(store_nbr, item_nbr, sep="_")
)]

data_wide <- dcast(train_data_f, store_item ~ date, value.var = "unit_sales", fill = 0)
``` 

```{r}
data_item <- data_wide %>% 
    filter(store_item == "44_1503844")


``` 

```{r}
  h <- 16
  frequency <- 7
  date_index_test <- tail(colnames(data_wide), 16)

  data_df <- melt(data_item, id.vars = c("store_item"))
  
  store_item_val <- data_df$store_item[1]
  data_df$store_item <- NULL
  
  
```  
  
```{r}
  colnames(data_df) <- c("ds", "y")
 
  # Date column handling
  data_ts <- data_df
  data_ts <- data_ts %>% 
                select(-ds)

 
  test_index <- tail(rownames(data_ts), h) %>% as.numeric()
  test_data <- data_ts %>% slice(test_index) %>% `row.names<-`(test_index)
  train_data <- data_ts %>% slice(-test_index)
  
  # Converts train data to time.series object
  train_ts <- ts(train_data, frequency = frequency, start = 1)
  
  
```  

## Arima forcast
```{r}
  fit_arima <- auto.arima(x = train_ts)
  fit_arima
```  

```{r}
    forecast_arima <- forecast(fit_arima, h = h)
    forecast_arima
```

## accuracy metrics
```{r}
    t_data <- test_data$y %>% as.numeric()
    accuracy_arima <- forecast::accuracy(forecast_arima, t_data)
    accuracy_arima
```  

## arima forecast
```{r}
   autoplot(forecast_arima)
``` 

## check residuals
```{r}
    checkresiduals(fit_arima)
``` 

##ACF plots
```{r}
ggAcf(train_ts, lag = 363)
ggAcf(train_ts, lag = 30)
ggAcf(train_ts, lag = 7)
```    
  
## prophet  
```{r}
    test_index <- tail(rownames(data_df), h) %>% as.numeric()
    test_data <- data_df %>% slice(test_index) %>% `row.names<-`(test_index)
    train_data <- data_df %>% slice(-test_index)
   
    
    df = train_data
    yearly.seasonality=TRUE
    weekly.seasonality = TRUE
    daily.seasonality=FALSE
    
    fit_prophet <- prophet(df = train_data, yearly.seasonality=TRUE, weekly.seasonality = TRUE,
                 daily.seasonality=FALSE)
    future <- make_future_dataframe(fit_prophet, periods = 16)
    forecast <- predict(fit_prophet, future)
    forecast <- forecast[c('ds', 'yhat')]
    forecast$store_item <- store_item_val
```   

## prophet forecast plot 
```{r}
plot(fit_prophet, forecast)
``` 

## prophet residuals plot
```{r}
  colnames(data_df) <- c("ds", "y")
  residuals_f = forecast['yhat'] - data_df['y']
  colnames(residuals_f) <- c("res")
  head(residuals_f)
  ggplot(residuals_f, aes(res)) + 
  geom_histogram( breaks = seq(-600, 600, by=100),
                  col="red", 
                  fill="green", 
                  alpha=.2)
  

``` 

```{r}
    forecast <- forecast[forecast$ds >= ymd("2017-07-30"),]
    
    t_data <- test_data$y %>% as.numeric()
    accuracy_prophet <- forecast::accuracy(forecast$yhat, t_data)
    accuracy_prophet
```  
## Accuracy measures (Multiple time series)
Accuracy measures for the 18310 time series considering Top 5 store numbers ( 44, 45, 47, 3, 49 )  by total sales as the baseline for processing multiple time series. 


-                  "mean_rmse",       "time_taken"
- mean         7.45920546451115,      "40.76 mins"
- ets          8.0311892817118,       "48.55 mins"
- auto.arima   6.589306454993,        "7.83 hours"
- prophet      7.49642904779891,      "3.24 hours"



 